{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000  train samples\n",
      "10000  train samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 111s - loss: 0.3323 - acc: 0.8990 - val_loss: 0.0768 - val_acc: 0.9763\n",
      "Test loss: 0.0768068430327\n",
      "Test accuracy 0.9763\n",
      "INFO:tensorflow:Restoring parameters from out/mnist_convnet.chkp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from out/mnist_convnet.chkp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 8 variables to const ops.\n",
      "81 ops in the final graph.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The following input nodes were not found: {'I'}\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5d55acee7412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0minput_node_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# an array of the input node(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0moutput_node_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# an array of output nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     tf.float32.as_datatype_enum)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# Save the optimized graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py\u001b[0m in \u001b[0;36moptimize_for_inference\u001b[0;34m(input_graph_def, input_node_names, output_node_names, placeholder_type_enum)\u001b[0m\n\u001b[1;32m    107\u001b[0m   optimized_graph_def = strip_unused_lib.strip_unused(\n\u001b[1;32m    108\u001b[0m       \u001b[0moptimized_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_node_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_node_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       placeholder_type_enum)\n\u001b[0m\u001b[1;32m    110\u001b[0m   optimized_graph_def = graph_util.remove_training_nodes(\n\u001b[1;32m    111\u001b[0m       optimized_graph_def, output_node_names)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/tools/strip_unused_lib.py\u001b[0m in \u001b[0;36mstrip_unused\u001b[0;34m(input_graph_def, input_node_names, output_node_names, placeholder_type_enum)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnot_found\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The following input nodes were not found: %s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnot_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   output_graph_def = graph_util.extract_sub_graph(inputs_replaced_graph_def,\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The following input nodes were not found: {'I'}\\n\""
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], ' train samples')\n",
    "print(x_test.shape[0], ' train samples')\n",
    "\n",
    "# convert class vectors to binary class matrix\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# building model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy', score[1])\n",
    "\n",
    "\n",
    "# export model\n",
    "model_name = 'mnist_convnet'\n",
    "\n",
    "file_path = 'out'\n",
    "input_graph_file = model_name + '.pbtxt'\n",
    "input_graph_file_path = file_path + '/' + input_graph_file\n",
    "checkpoint_file = model_name + '.chkp'\n",
    "checkpoint_file_path = file_path + '/' + checkpoint_file\n",
    "restore_op_name = \"save/restore_all\"\n",
    "filename_tensor_name = \"save/Const:0\"\n",
    "output_frozen_graph_file_path = file_path + '/frozen_' + model_name + '.pb'\n",
    "output_optimized_graph_file_path = file_path + '/optimized_' + model_name + '.pb'\n",
    "\n",
    "input_node_names = \"conv2d_1_input\"\n",
    "output_node_names = \"dense_2/Softmax\"\n",
    "\n",
    "if not path.exists(file_path):\n",
    "    os.mkdir(file_path)\n",
    "    \n",
    "# Save the graph\n",
    "tf.train.write_graph(K.get_session().graph_def, file_path, input_graph_file)\n",
    "\n",
    "# Save a checkpoint\n",
    "saver = tf.train.Saver()\n",
    "saver.save(K.get_session(), checkpoint_file_path)\n",
    "\n",
    "# Freeze the graph. convert variables in the checkpoint file into Const Ops\n",
    "freeze_graph.freeze_graph(input_graph_file_path, None, False, checkpoint_file_path,\n",
    "                          output_node_name, restore_op_name, filename_tensor_name, \n",
    "                          output_frozen_graph_file_path, True, \"\")\n",
    "\n",
    "# Optimize the model file\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.Open(output_frozen_graph_file_path, \"rb\") as f:\n",
    "    data = f.read()\n",
    "    input_graph_def.ParseFromString(data)\n",
    "    \n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "                    input_graph_def, \n",
    "                    [input_node_names], # an array of the input node(s)\n",
    "                    [output_node_names], # an array of output nodes\n",
    "                    tf.float32.as_datatype_enum)\n",
    "\n",
    "# Save the optimized graph\n",
    "with tf.gfile.FastGFile(output_optimized_graph_file_path, \"wb\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "    \n",
    "print (\"graph saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
